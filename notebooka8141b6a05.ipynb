{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10833432,"sourceType":"datasetVersion","datasetId":6727368}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-23T20:36:55.171974Z","iopub.execute_input":"2025-02-23T20:36:55.172247Z","iopub.status.idle":"2025-02-23T20:36:55.177841Z","shell.execute_reply.started":"2025-02-23T20:36:55.172225Z","shell.execute_reply":"2025-02-23T20:36:55.177022Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/the-liar-dataset/data.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nfrom tqdm.auto import tqdm\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim \nfrom sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizer , DistilBertModel\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom transformers import BertModel, BertTokenizer\nimport torch.nn as nn \nfrom torch.utils.data  import Dataset\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, accuracy_score\nimport joblib\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T20:36:55.185841Z","iopub.execute_input":"2025-02-23T20:36:55.186041Z","iopub.status.idle":"2025-02-23T20:37:01.372185Z","shell.execute_reply.started":"2025-02-23T20:36:55.186024Z","shell.execute_reply":"2025-02-23T20:37:01.371505Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/the-liar-dataset/data.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T20:37:01.373173Z","iopub.execute_input":"2025-02-23T20:37:01.373677Z","iopub.status.idle":"2025-02-23T20:37:02.666709Z","shell.execute_reply.started":"2025-02-23T20:37:01.373651Z","shell.execute_reply":"2025-02-23T20:37:02.665850Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T20:37:02.668125Z","iopub.execute_input":"2025-02-23T20:37:02.668342Z","iopub.status.idle":"2025-02-23T20:37:02.679009Z","shell.execute_reply.started":"2025-02-23T20:37:02.668323Z","shell.execute_reply":"2025-02-23T20:37:02.678295Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                               title  \\\n0   BREAKING: GOP Chairman Grassley Has Had Enoug...   \n1   Failed GOP Candidates Remembered In Hilarious...   \n2   Mike Pence’s New DC Neighbors Are HILARIOUSLY...   \n3  California AG pledges to defend birth control ...   \n4  AZ RANCHERS Living On US-Mexico Border Destroy...   \n\n                                                text       subject  \\\n0  Donald Trump s White House is in chaos, and th...          News   \n1  Now that Donald Trump is the presumptive GOP n...          News   \n2  Mike Pence is a huge homophobe. He supports ex...          News   \n3  SAN FRANCISCO (Reuters) - California Attorney ...  politicsNews   \n4  Twisted reasoning is all that comes from Pelos...      politics   \n\n               date  label  \n0     July 21, 2017      0  \n1       May 7, 2016      0  \n2  December 3, 2016      0  \n3  October 6, 2017       1  \n4      Apr 25, 2017      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>text</th>\n      <th>subject</th>\n      <th>date</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BREAKING: GOP Chairman Grassley Has Had Enoug...</td>\n      <td>Donald Trump s White House is in chaos, and th...</td>\n      <td>News</td>\n      <td>July 21, 2017</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Failed GOP Candidates Remembered In Hilarious...</td>\n      <td>Now that Donald Trump is the presumptive GOP n...</td>\n      <td>News</td>\n      <td>May 7, 2016</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Mike Pence’s New DC Neighbors Are HILARIOUSLY...</td>\n      <td>Mike Pence is a huge homophobe. He supports ex...</td>\n      <td>News</td>\n      <td>December 3, 2016</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>California AG pledges to defend birth control ...</td>\n      <td>SAN FRANCISCO (Reuters) - California Attorney ...</td>\n      <td>politicsNews</td>\n      <td>October 6, 2017</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AZ RANCHERS Living On US-Mexico Border Destroy...</td>\n      <td>Twisted reasoning is all that comes from Pelos...</td>\n      <td>politics</td>\n      <td>Apr 25, 2017</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"class SubjectDataset(Dataset):\n    def __init__(self , df):\n        self.df=df\n        self.maxlen=256\n        self.tokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\")\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self , index):\n        sample_title= str(self.df['title'].iloc[index])\n        sample_content= str(self.df['text'].iloc[index])\n        sample = sample_title + \" \" + sample_content \n        encodings = self.tokenizer.encode_plus(\n        sample,\n        add_special_tokens=True,\n        max_length=self.maxlen,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt' \n        )\n        label = torch.tensor(self.df['label'].iloc[index], dtype=torch.long)\n        return {\n            'input_ids': encodings['input_ids'].flatten(),  \n            'attention_mask': encodings['attention_mask'].flatten(),\n            'labels': label\n        } ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T20:37:02.680037Z","iopub.execute_input":"2025-02-23T20:37:02.680311Z","iopub.status.idle":"2025-02-23T20:37:02.694999Z","shell.execute_reply.started":"2025-02-23T20:37:02.680288Z","shell.execute_reply":"2025-02-23T20:37:02.694137Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def get_bert_embeddings(dataset, bert_model, device):\n    embeddings = [] # Will store BERT embeddings for each text\n    labels = [] ## for labels\n    bert_model = bert_model.to(device)\n    bert_model.eval()\n    \n    loader = DataLoader(dataset, batch_size=32, shuffle=False)\n    # Disable gradient calculations since we're only doing inference not classiftion \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Getting BERT embeddings\"):\n            input_ids = batch['input_ids'].to(device) ## batch to dvice [batch = 16 in this case _size, 256]\n            attention_mask = batch['attention_mask'].to(device) ## same here for attention_mask\n            \n            outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n            # Get [CLS] token embeddings\n            # Extract [CLS] token embeddings\n            # outputs.last_hidden_state shape: [batch_size, 256, 768]\n            # [:, 0, :] selects the first token ([CLS]) of each sequence\n            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n            embeddings.extend(batch_embeddings)\n            labels.extend(batch['labels'].cpu().numpy())\n        # Convert lists to numpy arrays\n    return np.array(embeddings), np.array(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T20:37:02.695775Z","iopub.execute_input":"2025-02-23T20:37:02.695967Z","iopub.status.idle":"2025-02-23T20:37:02.710103Z","shell.execute_reply.started":"2025-02-23T20:37:02.695950Z","shell.execute_reply":"2025-02-23T20:37:02.709225Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# def main():\n#     df = pd.read_csv(\"/kaggle/input/the-liar-dataset/data.csv\") ## for kaggek\n#     ## df = pd.read_csv(\"data.csv\") ## for lical\n#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n#     print(f'im using ',device)\n#     df_train , df_test = train_test_split(df,train_size=0.8,random_state=42)    \n#     df_train = SubjectDataset(df_train)\n#     df_test = SubjectDataset(df_test) \n#     # train_loader = DataLoader(\n#     #     df_train,\n#     #     batch_size=32,\n#     #     shuffle=True,\n#     # )\n\n#     # test_loader = DataLoader(\n#     #     df_test,\n#     #     batch_size=32,\n#     #     shuffle=False,\n#     # )\n#     bert_model = BertModel.from_pretrained('bert-base-uncased')\n#         # Get BERT embeddings\n#     print(\"getting train enbaging bert\")\n#     X_train, y_train = get_bert_embeddings(df_train, bert_model, device)\n#     print(\"getting test enbaging bert\")\n#     X_test, y_test = get_bert_embeddings(df_test, bert_model, device)\n#      # Train Random Forest\n#     print(\"Training Random Forest\")\n#     # param_grid = {\n#     # 'bootstrap': [True, False],\n#     # 'max_depth': [5, 10, 20, 30, 40, 50],\n#     # 'max_features': ['auto', 'sqrt', 'log2'],\n#     # 'min_samples_leaf': [1, 2, 4],\n#     # 'min_samples_split': [2, 5, 10],\n#     # 'n_estimators': [200, 400, 600, 800, 1000],\n#     # 'criterion': ['gini', 'entropy']\n#     #                 }\n#     param_grid = {\n#     'bootstrap': [True],  # Reduce options\n#     'max_depth': [10, 20, 30],  # Fewer depth options\n#     'max_features': ['auto'],  # Fewer feature selection methods\n#     'min_samples_leaf': [1, 2],\n#     'min_samples_split': [2, 5],\n#     'n_estimators': [200, 400, 600],\n#     'criterion': ['gini']\n#     }\n\n#     # Initialize base model\n#     rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)\n\n#     # Initialize GridSearchCV\n#     grid_search = GridSearchCV(\n#         estimator=rf_base,\n#         param_grid=param_grid,\n#         cv=5,                  # 5-fold cross-validation\n#         n_jobs=-1,            # Use all CPU cores\n#         verbose=2,            # Detailed output\n#         scoring='accuracy'    # Metric to optimize\n#     )\n#     print(\"Starting Grid Search...\")\n#     grid_search.fit(X_train, y_train)\n#     # Print best parameters and score\n#     print(\"\\nBest parameters found:\")\n#     print(grid_search.best_params_)\n#     print(f\"\\nBest cross-validation score: {grid_search.best_score_:.4f}\")\n#     # Evaluate on test set\n#     best_rf = grid_search.best_estimator_\n#     y_pred = best_rf.predict(X_test)\n#     print(\"\\nTest Set Performance:\")\n#     print(classification_report(y_test, y_pred))\n#      # Save model (optional)\n#     joblib.dump(best_rf, 'news_classifier_rf.joblib')\n#     return bert_model , best_rf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T20:37:02.710919Z","iopub.execute_input":"2025-02-23T20:37:02.711145Z","iopub.status.idle":"2025-02-23T20:37:02.727949Z","shell.execute_reply.started":"2025-02-23T20:37:02.711127Z","shell.execute_reply":"2025-02-23T20:37:02.727262Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def generate_bert_embeddings(df_path, model_name='bert-base-uncased'):\n    # Chargement des données\n    df = pd.read_csv(df_path)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Split des données\n    df_train, df_test = train_test_split(df, train_size=0.8, random_state=42)\n    \n    # Chargement du modèle BERT\n    bert_model = BertModel.from_pretrained(model_name)\n    \n    # Génération des embeddings\n    print(\"Génération des embeddings pour l'ensemble d'entraînement\")\n    X_train, y_train = get_bert_embeddings(SubjectDataset(df_train), bert_model, device)\n    \n    print(\"Génération des embeddings pour l'ensemble de test\")\n    X_test, y_test = get_bert_embeddings(SubjectDataset(df_test), bert_model, device)\n    \n    # Sauvegarde des embeddings\n    np.save('X_train_embeddings.npy', X_train)\n    np.save('y_train_labels.npy', y_train)\n    np.save('X_test_embeddings.npy', X_test)\n    np.save('y_test_labels.npy', y_test)\n    \n    return X_train, y_train, X_test, y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T20:37:02.728610Z","iopub.execute_input":"2025-02-23T20:37:02.728794Z","iopub.status.idle":"2025-02-23T20:37:02.748340Z","shell.execute_reply.started":"2025-02-23T20:37:02.728777Z","shell.execute_reply":"2025-02-23T20:37:02.747609Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def train_random_forest(X_train, y_train, X_test, y_test):\n    # Espace de recherche très réduit\n    param_grid = {\n        'n_estimators': [200, 400],\n        'max_depth': [10, 20],\n        'min_samples_split': [2, 5],\n        'criterion': ['gini']\n    }\n    \n    # Modèle de base\n    rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)\n    \n    # Recherche par grille\n    grid_search = GridSearchCV(\n        estimator=rf_base,\n        param_grid=param_grid,\n        cv=3,  # Réduction du nombre de plis\n        n_jobs=-1,\n        verbose=1,\n        scoring='accuracy'\n    )\n    \n    # Entraînement\n    grid_search.fit(X_train, y_train)\n    \n    # Meilleur modèle\n    best_rf = grid_search.best_estimator_\n    \n    # Évaluation\n    print(\"\\nMeilleurs paramètres :\")\n    print(grid_search.best_params_)\n    print(f\"\\nMeilleur score de validation croisée : {grid_search.best_score_:.4f}\")\n    \n    # Prédictions sur l'ensemble de test\n    y_pred = best_rf.predict(X_test)\n    print(\"\\nPerformance sur l'ensemble de test :\")\n    print(classification_report(y_test, y_pred))\n    \n    # Sauvegarde du modèle\n    joblib.dump(best_rf, 'news_classifier_rf.joblib')\n    \n    return best_rf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T20:37:02.749837Z","iopub.execute_input":"2025-02-23T20:37:02.750068Z","iopub.status.idle":"2025-02-23T20:37:02.765868Z","shell.execute_reply.started":"2025-02-23T20:37:02.750049Z","shell.execute_reply":"2025-02-23T20:37:02.765124Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def main():\n    # Chemin vers votre fichier CSV\n    df_path = \"/kaggle/input/the-liar-dataset/data.csv\"\n    \n    # Génération des embeddings\n    X_train, y_train, X_test, y_test = generate_bert_embeddings(df_path)\n    \n    # Entraînement du Random Forest\n    best_model = train_random_forest(X_train, y_train, X_test, y_test)\n    \n    return best_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T20:37:02.767012Z","iopub.execute_input":"2025-02-23T20:37:02.767292Z","iopub.status.idle":"2025-02-23T20:37:02.784219Z","shell.execute_reply.started":"2025-02-23T20:37:02.767262Z","shell.execute_reply":"2025-02-23T20:37:02.783268Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    bert_model, rf_model = main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-23T20:51:27.436694Z","iopub.execute_input":"2025-02-23T20:51:27.436916Z","iopub.status.idle":"2025-02-23T21:33:38.652660Z","shell.execute_reply.started":"2025-02-23T20:51:27.436894Z","shell.execute_reply":"2025-02-23T21:33:38.650209Z"}},"outputs":[{"name":"stdout","text":"Génération des embeddings pour l'ensemble d'entraînement\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Getting BERT embeddings:   0%|          | 0/1123 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ca2c61da8bb43398f7666e258bd2769"}},"metadata":{}},{"name":"stdout","text":"Génération des embeddings pour l'ensemble de test\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Getting BERT embeddings:   0%|          | 0/281 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37eb04f94fe8448fb424eb63639d65c1"}},"metadata":{}},{"name":"stdout","text":"Fitting 3 folds for each of 8 candidates, totalling 24 fits\n\nMeilleurs paramètres :\n{'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 400}\n\nMeilleur score de validation croisée : 0.9558\n\nPerformance sur l'ensemble de test :\n              precision    recall  f1-score   support\n\n           0       0.96      0.96      0.96      4669\n           1       0.96      0.96      0.96      4311\n\n    accuracy                           0.96      8980\n   macro avg       0.96      0.96      0.96      8980\nweighted avg       0.96      0.96      0.96      8980\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-d17502db3aa3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"],"ename":"ValueError","evalue":"too many values to unpack (expected 2)","output_type":"error"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}